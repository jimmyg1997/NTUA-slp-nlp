{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import nltk\n",
    "from shutil import copyfile\n",
    "\n",
    "#SSL Certificate has fauled\n",
    "#that not in the system certificate store.\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "#PlaintextCorpusReader will use the default nltk.tokenize.sent_tokenize() \n",
    "#and nltk.tokenize.word_tokenize() to split your texts into sentences and words\n",
    "\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 1----------------------------------------------------------#\n",
    "#Text number 1661 is \"The Adventures of Sherlock Holmes\" by Arthur Conan Doyle, and we can access it as follows.\n",
    "url = \"http://www.gutenberg.org/cache/epub/1661/pg1661.txt\"\n",
    "response = request.urlopen(url)\n",
    "corpus = response.read().decode('utf8')\n",
    "corpus = corpus.replace('\\r', '')\n",
    "length_corpus = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newcorpus.nosync/spell_checker_test_set.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make new dir for the corpus.\n",
    "corpusdir = 'newcorpus.nosync/'\n",
    "if not os.path.isdir(corpusdir):\n",
    "    os.mkdir(corpusdir)\n",
    "    \n",
    "copyfile(\"Makefile\", corpusdir + \"Makefile\")\n",
    "copyfile(\"spell_checker_test_set.txt\",corpusdir + \"spell_checker_test_set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the files into the directory.\n",
    "filename = 'SherlockHolmes.txt'\n",
    "with open(corpusdir+filename, 'w') as f:\n",
    "    print(corpus, file=f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that our corpus do exist and the files are correct.\n",
    "# Key Note:\n",
    "# 1.We split each file into words and we their equality until the penultimate word, since there is one extra '\\n'\n",
    "#in the created file\n",
    "assert open(corpusdir+filename,'r').read().split(' ')[:-1] == corpus.split(' ')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new corpus by specifying the parameters\n",
    "# (1) directory of the new corpus\n",
    "# (2) the fileids of the corpus\n",
    "# NOTE: in this case the fileids are simply the filenames.\n",
    "# Now the text has been parsed into paragraphs, sentences and words by the default actions\n",
    "# of the PlaintextCorpusReader\n",
    "newcorpus = PlaintextCorpusReader(corpusdir, '.*')\n",
    "os.chdir(corpusdir)\n",
    "#----------------------------------------------END OF STEP 1---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 2----------------------------------------------------------#\n",
    "#----------------------(a)---------------------#\n",
    "#Function used as default argument in parser() function if it is not defined\n",
    "def identity_preprocess(s):\n",
    "    if(isinstance(s, str)):\n",
    "        return s\n",
    "    else: return \"No string was given\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------(b)---------------------#\n",
    "#Function to parse the text file given, line by line\n",
    "def parser(path,preprocess = identity_preprocess):\n",
    "    tokens = []\n",
    "    for line in path.split('\\n'):\n",
    "        tokens+= preprocess(line)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------(c)---------------------#\n",
    "import re\n",
    "import string\n",
    "#Tokenization step, a simple version which includes tokens of lowercase words\n",
    "def tokenize(s):\n",
    "    s_temp = s.strip().lower()\n",
    "    s_temp = re.sub('[^A-Za-z\\n\\s]+', '', s_temp)\n",
    "    s_temp = s_temp.replace('\\n', ' ')\n",
    "    s_temp = \" \".join(s_temp.split())\n",
    "    s_temp = s_temp.split(' ')\n",
    "    s_temp[:] = [item for item in s_temp if item != '']\n",
    "    return s_temp\n",
    "#----------------------------------------------END OF STEP 2---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 3----------------------------------------------------------#\n",
    "#Constructing word tokens and alphabet of the new corpus\n",
    "#----------------------(a)---------------------#\n",
    "corpus_preprocessed = newcorpus.raw(newcorpus.fileids()[1])\n",
    "word_tokens = parser(corpus_preprocessed, tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------(b)---------------------#\n",
    "def tokenize_2(s):\n",
    "    s_temp = s.strip()\n",
    "    s_temp = \" \".join(s_temp.split())\n",
    "    s_temp = s_temp.split(' ')\n",
    "    return s_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_2(path, preprocess):\n",
    "    alphabet = []\n",
    "    for line in path.split('\\n'):\n",
    "        line = preprocess(line)\n",
    "        for word in line:\n",
    "            alphabet+= list(word)\n",
    "            \n",
    "    alphabet.append(' ')\n",
    "    return set(alphabet)\n",
    "        \n",
    "alphabet_tokens = sorted(parser_2(corpus_preprocessed,tokenize_2))\n",
    "#----------------------------------------------END OF STEP 3---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 4----------------------------------------------------------#\n",
    "filename = 'chars.syms'\n",
    "filename =  open(filename, 'w')\n",
    "result = []\n",
    "\n",
    "filename.write('<epsilon>'+ \" \" + str(0)+'\\n')\n",
    "filename.write('<space>'+ \"   \" + str(1)+'\\n')\n",
    "for symbol in range(2,len(alphabet_tokens)):\n",
    "    line = alphabet_tokens[symbol] + \"         \" + str(symbol)+'\\n'\n",
    "    filename.write(line)\n",
    "\n",
    "filename.close()\n",
    "#----------------------------------------------END OF STEP 4---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 10----------------------------------------------------------#\n",
    "from collections import defaultdict\n",
    "def create_words_dictionary(word_tokens):\n",
    "    length = len(word_tokens)\n",
    "    wordfreq = defaultdict(float)\n",
    "    for i in range(len(word_tokens)):\n",
    "        wordfreq[word_tokens[i]] += 1/length \n",
    "    return wordfreq\n",
    "\n",
    "words_dictionary = create_words_dictionary(word_tokens)\n",
    "#for k, v in words_dictionary.items():\n",
    "#    print(k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_characters_dictionary(alphabet_tokens, corpus_preprocessed):\n",
    "    result = {}\n",
    "    length = len(corpus_preprocessed) - corpus_preprocessed.count('\\n')\n",
    "    charfreq = [corpus_preprocessed.count(symbol)/length for symbol in alphabet_tokens ]\n",
    "    return dict(zip(alphabet_tokens,charfreq))\n",
    "\n",
    "characters_dictionary = create_characters_dictionary(alphabet_tokens, corpus_preprocessed)\n",
    "#for k, v in characters_dictionary.items():\n",
    "#    print(k, v)\n",
    "#----------------------------------------------END OF STEP 10---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 11----------------------------------------------------------#\n",
    "#Calculating the costs of transition for each word as cost_w_i = -log(p(w_i))\n",
    "#and after that the mean value\n",
    "#----------------------(a)---------------------#\n",
    "import math\n",
    "import statistics \n",
    "words_dictionary_costs = dict(zip(list(set(word_tokens)),[-math.log10(value) for key, value in words_dictionary.items()]))\n",
    "\n",
    "costs = [words_dictionary_costs[key] for key in words_dictionary_costs]\n",
    "w = statistics.mean(costs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE CREATE THE TRANDUCER I\n",
    "#for the word_tokens\n",
    "#----------------------(b)---------------------#\n",
    "filename = 'orth_I_words.txt'\n",
    "filename = open(filename,'w')\n",
    "\n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "for letter in alphabet:\n",
    "    filename.write(\"0 0 \"+ letter +\" \"+ letter +\" 0\\n\")\n",
    "filename.write(\"0\")\n",
    "\n",
    "filename.close()\n",
    "!make -s orth_I_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE CREATE THE TRANDUCER E\n",
    "filename = 'orth_E_words.txt'\n",
    "filename = open(filename,'w')\n",
    "filename.write('0 1 <epsilon> <epsilon> 0'+'\\n')\n",
    "for i in range(len(alphabet)):\n",
    "    filename.write('0 1 <epsilon> '+alphabet[i]+' '+str(w)+'\\n')#insertion\n",
    "    filename.write('0 1 ' + alphabet[i]+' <epsilon> '+str(w)+'\\n')#deletion\n",
    "    for j in range(len(alphabet)):\n",
    "        if alphabet[i]!=alphabet[j]:\n",
    "            filename.write('0 1 ' + alphabet[i]+' '+alphabet[j]+' '+str(w)+'\\n')#Replace character by another\n",
    "\n",
    "filename.write(str(1))\n",
    "filename.close()\n",
    "!make -s orth_E_words\n",
    "!make -s transducer_words\n",
    "!make -s transducershortest_words\n",
    "#FINALLY WE CREATE THE TRANDUCER transducer = orth_I | orth_E | orth_I with the Makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.215014865006331\n"
     ]
    }
   ],
   "source": [
    "#----------------------(c)---------------------#\n",
    "#Calculating the costs of transition for each char as cost_c_i = -log(p(c_i))\n",
    "#and after that the mean value\n",
    "characters_dictionary_costs = dict(zip(list(set(alphabet_tokens)),[-math.log10(value) for key, value in characters_dictionary.items()]))\n",
    "costs = [characters_dictionary_costs[key] for key in characters_dictionary_costs]\n",
    "w = statistics.mean(costs)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE CREATE THE TRANDUCER I\n",
    "#for the char_tokens\n",
    "filename = 'orth_I_chars.txt'\n",
    "filename = open(filename,'w')\n",
    "\n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "for letter in alphabet:\n",
    "    filename.write(\"0 0 \"+ letter +\" \"+ letter +\" 0\\n\")\n",
    "filename.write(\"0\")\n",
    "\n",
    "filename.close()\n",
    "!make -s orth_I_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE CREATE THE TRANDUCER E\n",
    "filename = 'orth_E_chars.txt'\n",
    "filename = open(filename,'w')\n",
    "filename.write('0 1 <epsilon> <epsilon> 0'+'\\n')\n",
    "for i in range(len(alphabet)):\n",
    "    filename.write('0 1 <epsilon> '+alphabet[i]+' '+str(w)+'\\n')#insertion\n",
    "    filename.write('0 1 ' + alphabet[i]+' <epsilon> '+str(w)+'\\n')#deletion\n",
    "    for j in range(len(alphabet)):\n",
    "        if alphabet[i]!=alphabet[j]:\n",
    "            filename.write('0 1 ' + alphabet[i]+' '+alphabet[j]+' '+str(w)+'\\n')#Replace character by another\n",
    "\n",
    "filename.write(str(1))\n",
    "filename.close()\n",
    "!make -s orth_E_chars\n",
    "!make -s transducer_chars\n",
    "#FINALLY WE CREATE THE TRANDUCER transducer = orth_I | orth_E | orth_I with the Makefile\n",
    "#----------------------------------------------END OF STEP 11---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 12----------------------------------------------------------#\n",
    "#HERE WE CREATE THE ACCEPTOR/AUTOMATO used to accept all the words of our words_tokens, of the corpus.\n",
    "#One state for each letter of every word-> States will be limited later when we will apply the respective\n",
    "#commands of determinization, minimization, removal of <epsilon> transitions to our orth_acceptor.fst\n",
    "#----------------------(a)---------------------#\n",
    "filename = 'orth_acceptor_words.txt'\n",
    "acceptor=open(filename, 'w')\n",
    "final_states = []\n",
    "state_count = 0\n",
    "\n",
    "acceptor.write('0 0 <epsilon> 0\\n')\n",
    "\n",
    "for word in list(set(word_tokens)):\n",
    "    chars = list(word)\n",
    "    if(len(chars) == 1):\n",
    "        arg = ['0',' ',str(state_count+1),' ',chars[0],' ',chars[0],' ', str(words_dictionary_costs[word]),'\\n']\n",
    "        arg = ''.join(arg)\n",
    "        acceptor.write(arg)\n",
    "        state_count += len(chars)\n",
    "        final_states.append(str(state_count))\n",
    "    else:\n",
    "        arg = ['0',' ',str(state_count+1),' ',chars[0],' ',chars[0],' ',str(words_dictionary_costs[word]),'\\n']\n",
    "        arg = ''.join(arg)\n",
    "        acceptor.write(arg)\n",
    "        for j in range(1,len(chars)):\n",
    "            arg = [str(j + state_count),' ',str(j+1 + state_count),' ',chars[j],' ',chars[j],' 0','\\n']\n",
    "            arg = ''.join(arg)\n",
    "            acceptor.write(arg)\n",
    "        state_count += len(chars)\n",
    "        final_states.append(str(state_count))\n",
    "for i in range(0,len(final_states)):\n",
    "    arg = [final_states[i],'\\n']\n",
    "    arg = ''.join(arg)\n",
    "    acceptor.write(arg)\n",
    "acceptor.close()\n",
    "!make -s orth_acceptor_words\n",
    "!make -s orth_acceptor_processed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE CREATE THE ACCEPTOR/AUTOMATO used to accept all the words of our char_tokens, of the corpus.\n",
    "#One state for each letter -> States will be limited later when we will apply the respective\n",
    "#commands of determinization, minimization, removal of <epsilon> transitions to our orth_acceptor.fst\n",
    "#----------------------(b)---------------------#\n",
    "filename = 'orth_acceptor_chars.txt'\n",
    "acceptor=open(filename, 'w')\n",
    "final_states = []\n",
    "state_count = 0\n",
    "\n",
    "acceptor.write('0 0 <epsilon> 0\\n')\n",
    "for word in list(set(word_tokens)):\n",
    "    chars = list(word)\n",
    "    if(len(chars) == 1):\n",
    "        arg = ['0',' ',str(state_count+1),' ',chars[0],' ',chars[0],' ', str(characters_dictionary_costs[chars[0]]),'\\n']\n",
    "        arg = ''.join(arg)\n",
    "        acceptor.write(arg)\n",
    "        state_count += len(chars)\n",
    "        final_states.append(str(state_count))\n",
    "    else:\n",
    "        arg = ['0',' ',str(state_count+1),' ',chars[0],' ',chars[0],' ',str(characters_dictionary_costs[chars[0]]),'\\n']\n",
    "        arg = ''.join(arg)\n",
    "        acceptor.write(arg)\n",
    "        for j in range(1,len(chars)):\n",
    "            arg = [str(j + state_count),' ',str(j+1 + state_count),' ',chars[j],' ',chars[j],' ',str(characters_dictionary_costs[chars[j]]),'\\n']\n",
    "            arg = ''.join(arg)\n",
    "            acceptor.write(arg)\n",
    "        state_count += len(chars)\n",
    "        final_states.append(str(state_count))\n",
    "for i in range(0,len(final_states)):\n",
    "    arg = [final_states[i],'\\n']\n",
    "    arg = ''.join(arg)\n",
    "    acceptor.write(arg)\n",
    "acceptor.close()\n",
    "!make -s orth_acceptor_chars\n",
    "!make -s orth_acceptor_processed_chars\n",
    "\n",
    "#----------------------------------------------END OF STEP 12---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the word <cit> with the orthograph_words\n",
      "p\n",
      "0\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 13----------------------------------------------------------#\n",
    "#----------------------(a)---------------------#\n",
    "!make -s orthograph_words\n",
    "\n",
    "#----------------------(b)---------------------#\n",
    "!make -s orthograph_chars\n",
    "\n",
    "#----------------------(c)---------------------#\n",
    "filename = 'cit.txt'\n",
    "filename = open(filename, 'w')\n",
    "word = \"cit\"\n",
    "state = 0\n",
    "for letter in word:\n",
    "    if letter!='\\n':\n",
    "        filename.write(str(state)+' '+str(state+1)+' '+letter+ '\\n')\n",
    "        state+=1\n",
    "filename.write(str(state)+'\\n')\n",
    "\n",
    "filename.close()\n",
    "print(\"Checking the word <cit> with the orthograph_words\")\n",
    "!make -s check_cit_words\n",
    "print(\"Checking the word <cit> with the orthograph_chars\")\n",
    "!make -s check_cit_chars\n",
    "\n",
    "#----------------------------------------------END OF STEP 13---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 14----------------------------------------------------------#\n",
    "from lib import *\n",
    "filename = 'spell_checker_test_set.txt'\n",
    "#We take 'spell_checker_test_set.txt', and we split to create 2 lists, the one with the correct words\n",
    "#and the other with the list of the relevant wrong words. We chose randomly to ckeck 20 lines \n",
    "filename = open(filename, 'r')\n",
    "lines = filename.readlines()\n",
    "correct_words = []\n",
    "wrong_words =[]\n",
    "for line in lines:\n",
    "    correct_words.append(line.split(':')[0])\n",
    "    wrong_words.append((line.split(':')[1]).split())\n",
    "\n",
    "acceptor = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should create the dictionary based on the \"chars.syms\". The position in the dictionary\n",
    "#represents the index in the symbol\n",
    "dictionary = 'chars.syms'\n",
    "dictionary= open(dictionary,'r')\n",
    "lines=dictionary.readlines()\n",
    "dict=[0 for i in range(len(lines))]\n",
    "for line in lines:\n",
    "    matching = line.split()\n",
    "    dict[int(matching[1])]=matching[0]\n",
    "dictionary.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here in file OurResults, we will save the produced words\n",
    "filename_words = 'OurResults_words.txt'\n",
    "filename_chars = 'OurResults_chars.txt'\n",
    "result_words = open(filename_words, 'w')\n",
    "result_chars = open(filename_chars, 'w')\n",
    "for i in range(len(wrong_words)):\n",
    "    for word in wrong_words[i]:\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #We truncate this file in order to make the other acceptors in the same file\n",
    "        acceptor=open('word_acceptor.txt', 'w')\n",
    "        state = 0\n",
    "        for letter in word:\n",
    "            if letter!='\\n':\n",
    "                acceptor.write(str(state)+' '+str(state+1)+' '+letter +'\\n')\n",
    "                \n",
    "                state+=1\n",
    "        acceptor.write(str(state)+'\\n')\n",
    "        acceptor.close()\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #We use the fst tool in order to create the acceptor for every word\n",
    "        #The method of shortest path was used to find the best matches\n",
    "        !make -s unique_word\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #We write the result in a file in order to compare the best words later\n",
    "        acceptor_shortest_words=open('Acceptor_Shortest_words.txt', 'r')\n",
    "        lines=acceptor_shortest_words.readlines()\n",
    "        temp_word=[]\n",
    "\n",
    "        for j in range(2,len(lines)):\n",
    "            chars = lines[j].split()\n",
    "            if(len(chars) > 3):\n",
    "                temp_word.append(chars[3])\n",
    "        if(len(lines) > 1):\n",
    "            chars = lines[0].split()\n",
    "            if(len(chars) > 3):\n",
    "                temp_word.append(chars[3])\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #Apparently, now in temp_word we have the produced word, which is going to be\n",
    "        #cheked based on our dictionary created in the previous block.\n",
    "        for letter in temp_word[1:(len(temp_word)-1)]:\n",
    "            if int(letter)!=0:\n",
    "                result_words.write(dict[int(letter)])\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #So for each word we save our result bh using this format:\n",
    "        #|word orthograph| + |wrong_word| + |correct_word|\n",
    "        result_words.write(' '+word+' '+correct_words[i]+'\\n')\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #Repeat the procedure for the orthograph_chars\n",
    "        acceptor_shortest_chars=open('Acceptor_Shortest_chars.txt', 'r')\n",
    "        lines=acceptor_shortest_chars.readlines()\n",
    "        temp_word=[]\n",
    "        for j in range(2,len(lines)):\n",
    "            chars = lines[j].split()\n",
    "            if(len(chars) > 3):\n",
    "                temp_word.append(chars[3])\n",
    "        if(len(lines) > 1):\n",
    "            chars = lines[0].split()\n",
    "            if(len(chars) > 3):\n",
    "                temp_word.append(chars[3])\n",
    "                \n",
    "        for letter in temp_word[1:(len(temp_word)-1)]:\n",
    "            if int(letter)!=0:\n",
    "                result_chars.write(dict[int(letter)])\n",
    "        result_chars.write(' '+word+' '+correct_words[i]+'\\n')\n",
    "        \n",
    "        \n",
    "\n",
    "result_words.close()\n",
    "result_chars.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HERE WE GONNA CHECK THE CORRECTNESS OF OUR ORTHOGRAPH_WORDS\n",
    "corrected_words=0\n",
    "wrong_words=0\n",
    "no_matching_words=0\n",
    "result=open('OurResults_words.txt', 'r')\n",
    "words=result.readlines()\n",
    "for word in words:\n",
    "    chars = word.split()\n",
    "    if(len(chars) >2):\n",
    "        if(chars[0] == chars[2] and chars[1]!=chars[2]):\n",
    "            corrected_words+=1\n",
    "        else:\n",
    "            wrong_words +=1\n",
    "    else:\n",
    "        no_matching_words+=1\n",
    "\n",
    "print('\\nCHECKING WITH ORTHOGRAPH_WORDS GAVE THE FOLLOWING RESULTS\\n')\n",
    "print('Corrected Words ' + str(corrected_words))\n",
    "print('Wrong Words ' + str(wrong_words))\n",
    "print('There was no matching for '+ str(no_matching_words) + ' words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### HERE WE GONNA CHECK THE CORRECTNESS OF OUR ORTHOGRAPH_CHARS\n",
    "corrected_words=0\n",
    "wrong_words=0\n",
    "no_matching_words=0\n",
    "result=open('OurResults_chars.txt', 'r')\n",
    "words=result.readlines()\n",
    "for word in words:\n",
    "    chars = word.split()\n",
    "    if(len(chars) >2):\n",
    "        if(chars[0] == chars[2] and chars[1]!=chars[2]):\n",
    "            corrected_words+=1\n",
    "        else:\n",
    "            wrong_words +=1\n",
    "    else:\n",
    "        no_matching_words+=1\n",
    "\n",
    "print('\\nCHECKING WITH ORTHOGRAPH_CHARS GAVE THE FOLLOWING RESULTS\\n')\n",
    "print('Corrected Words ' + str(corrected_words))\n",
    "print('Wrong Words ' + str(wrong_words))\n",
    "print('There was no matching for '+ str(no_matching_words) + ' words')\n",
    "#----------------------------------------------END OF STEP 14---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 15----------------------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
