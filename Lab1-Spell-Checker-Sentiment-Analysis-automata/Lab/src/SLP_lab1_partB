{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 16----------------------------------------------------------#\n",
    "import os, os.path\n",
    "import nltk\n",
    "from shutil import copyfile\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'aclImdb/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "pos_train_dir = os.path.join(train_dir, 'pos')\n",
    "neg_train_dir = os.path.join(train_dir, 'neg')\n",
    "pos_test_dir = os.path.join(test_dir, 'pos')\n",
    "neg_test_dir = os.path.join(test_dir, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory limitations. These parameters fit in 8GB of RAM.\n",
    "# If you have 16G of RAM you can experiment with the full dataset / W2V\n",
    "MAX_NUM_SAMPLES = 5000\n",
    "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
    "# sorted from most frequent to least frequent.\n",
    "# It may yield much worse results for other embeddings corpora\n",
    "NUM_W2V_TO_LOAD = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEED = 42\n",
    "# Fix numpy random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "try:\n",
    "    import glob2 as glob\n",
    "except ImportError:\n",
    "    import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strip_punctuation(s):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', ' ', s)\n",
    "\n",
    "def preprocess(s):\n",
    "    return re.sub('\\s+',' ', strip_punctuation(s).lower())\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "def preproc_tok(s):\n",
    "    return tokenize(preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samples(folder, preprocess=lambda x: x):\n",
    "    samples = glob.iglob(os.path.join(folder, '*.txt'))\n",
    "    data = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
    "            break\n",
    "        with open(sample, 'r') as fd:\n",
    "            x = [preprocess(l) for l in fd][0]\n",
    "            data.append(x)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(pos, neg):\n",
    "    corpus = np.array(pos + neg)\n",
    "    y = np.array([1 for _ in pos] + [0 for _ in neg])\n",
    "    indices = np.arange(y.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return list(corpus[indices]), list(y[indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the prementioned funtions\n",
    "pos_train = read_samples(pos_train_dir,preprocess)\n",
    "neg_train = read_samples(neg_train_dir,preprocess)\n",
    "corpus_train, indices_train = create_corpus(pos_train,neg_train)\n",
    "#----------------------------------------------END OF STEP 16---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW's Training error =  0.0003\n",
      "BOW's Accuracy =  0.8567\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 17----------------------------------------------------------#\n",
    "#----------------------(b)---------------------#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import zero_one_loss, accuracy_score\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(corpus_train)\n",
    "#----------------------(c)---------------------#\n",
    "\n",
    "#Training the logistic regression model\n",
    "clf = LogisticRegression().fit(X_train_counts, indices_train)\n",
    "print(\"BOW's Training error = \", zero_one_loss(indices_train,clf.predict(X_train_counts)))\n",
    "\n",
    "#Predicting for the test reviews if they are positive or negative based on the previous trained model\n",
    "pos_test = read_samples(pos_test_dir,preprocess)\n",
    "neg_test = read_samples(neg_test_dir,preprocess)\n",
    "corpus_test, indices_test = create_corpus(pos_test,neg_test)\n",
    "\n",
    "X_test_counts = count_vect.transform(corpus_test)\n",
    "count_predictions = clf.predict(X_test_counts)\n",
    "print(\"BOW's Accuracy = \", accuracy_score(indices_test,count_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfTransformer's Training error =  0.0538\n",
      "TfidfTransformer's Accuracy =  0.8719\n"
     ]
    }
   ],
   "source": [
    "#----------------------(d)---------------------#\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vect.fit_transform(corpus_train)\n",
    "\n",
    "#Training the logistic regression model\n",
    "clf = LogisticRegression().fit(X_train_tfidf, indices_train)\n",
    "print(\"TfidfTransformer's Training error = \", zero_one_loss(indices_train,clf.predict(X_train_tfidf)))\n",
    "\n",
    "X_test_tfidf = tfidf_vect.transform(corpus_test)\n",
    "tfidf_predictions = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"TfidfTransformer's Accuracy = \", accuracy_score(indices_test,tfidf_predictions))\n",
    "\n",
    "#----------------------------------------------END OF STEP 17---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4570\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 18----------------------------------------------------------#\n",
    "#----------------------(a)---------------------#\n",
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def identity_preprocess(s):\n",
    "    if(isinstance(s, str)):\n",
    "        return s\n",
    "    else: return \"No string was given\"\n",
    "\n",
    "def parser(path,preprocess = identity_preprocess):\n",
    "    tokens = []\n",
    "    for line in path.split('\\n'):\n",
    "        s_temp = preprocess(line)\n",
    "        if(s_temp ==[]):continue\n",
    "        tokens.append(s_temp)\n",
    "    return tokens\n",
    "\n",
    "def tokenize(s):\n",
    "    s_temp = s.strip().lower()\n",
    "    s_temp = re.sub('[^A-Za-z\\n\\s]+', '', s_temp)\n",
    "    s_temp = s_temp.replace('\\n', ' ')\n",
    "    s_temp = \" \".join(s_temp.split())\n",
    "    s_temp = s_temp.split(' ')\n",
    "    s_temp[:] = [item for item in s_temp if item != '']\n",
    "    return s_temp\n",
    "\n",
    "url = \"http://www.gutenberg.org/cache/epub/1661/pg1661.txt\"\n",
    "response = request.urlopen(url)\n",
    "corpus = response.read().decode('utf8')\n",
    "corpus = corpus.replace('\\r', '')\n",
    "\n",
    "sent_tokens = parser(corpus,tokenize)\n",
    "\n",
    "model = models.Word2Vec( sent_tokens,window=5, size=100, min_count = 2,workers=4)\n",
    "model.train(sent_tokens, total_examples=len(sent_tokens), epochs=1000)\n",
    "\n",
    "# get ordered vocabulary list|\n",
    "voc = model.wv.index2word\n",
    "# get vector size| dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of OOV is:  43.356470004957856  %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_tokens = preproc_tok(corpus)\n",
    "length = len(list(set(word_tokens)))\n",
    "print(\"The percentage of OOV is: \" ,(length - len(voc))*100/length,\" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec's Accuracy =  0.6084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#----------------------(b)---------------------#\n",
    "# syn0 is a numpy array that stores a feature vector for each word.\n",
    "from functools import partial\n",
    "\n",
    "# First method - averaging words vectors.\n",
    "def ws2meanvec(words, embedding_model):\n",
    "    vectors = [embedding_model[w] for w in words if w in embedding_model]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # To avoid empty words causing error. For example, raw sentence is 10/10 at imdb.\n",
    "        return np.zeros(embedding_model.vector_size, dtype=np.float32)\n",
    "\n",
    "\n",
    "X_train_tl = list(map(partial(ws2meanvec, embedding_model=model), corpus_train))\n",
    "X_test_tl = list(map(partial(ws2meanvec, embedding_model=model), corpus_test))\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X_train_tl, indices_train)\n",
    "pred_test = log_reg.predict(X_test_tl)\n",
    "print(\"Word2Vec's Accuracy = \", accuracy_score(indices_test, pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how [('what', 0.6820360422134399), ('How', 0.6297600269317627), ('why', 0.5838741064071655)]\n",
      "surprised [('shocked', 0.8090525269508362), ('flabbergasted', 0.7832474708557129), ('taken_aback', 0.7816465497016907)]\n",
      "smokes [('smokes_cigarettes', 0.69474858045578), ('cigarettes', 0.6854457259178162), ('smoked', 0.6450352668762207)]\n",
      "savagely [('viciously', 0.7358489036560059), ('brutally', 0.7070984244346619), ('mercilessly', 0.6742576956748962)]\n",
      "she [('her', 0.7834683656692505), ('She', 0.7553189396858215), ('herself', 0.669890820980072)]\n",
      "stooped [('stooping', 0.6364398002624512), ('stoop', 0.6238372921943665), ('stoops', 0.5228399038314819)]\n",
      "penknife [('pocketknife', 0.6871023178100586), ('knife', 0.6770156621932983), ('knives', 0.6004471778869629)]\n",
      "lay [('laying', 0.771097719669342), ('laid', 0.7509710192680359), ('Laying', 0.6501108407974243)]\n",
      "detail [('details', 0.6135907173156738), ('detailed', 0.5740132331848145), ('specifics', 0.5441097021102905)]\n",
      "fierce [('ferocious', 0.7291355133056641), ('intense', 0.6843384504318237), ('fiercest', 0.6470801830291748)]\n"
     ]
    }
   ],
   "source": [
    "#----------------------(c)(d)---------------------#\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load google pre-trained model.\n",
    "google_model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True, limit=NUM_W2V_TO_LOAD)\n",
    "\n",
    "X_train_tl = list(map(partial(ws2meanvec, embedding_model=google_model), corpus_train))\n",
    "X_test_tl = list(map(partial(ws2meanvec, embedding_model=google_model), corpus_test))\n",
    "\n",
    "\n",
    "words_to_check = ['how','surprised','smokes','savagely','she','stooped','penknife','lay','detail','fierce']\n",
    "for word in words_to_check:\n",
    "    sim = google_model.wv.most_similar(word,topn=3)\n",
    "    print(word,sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodle Word2Vec's Accuracy =  0.6041\n"
     ]
    }
   ],
   "source": [
    "#----------------------(e)---------------------#\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X_train_tl, indices_train)\n",
    "pred_test = log_reg.predict(X_test_tl)\n",
    "print(\"Goodle Word2Vec's Accuracy = \", accuracy_score(indices_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodle Word2Vec TF-IDF's Accuracy =  0.5\n"
     ]
    }
   ],
   "source": [
    "#----------------------(f)(z)---------------------#\n",
    "tfidf = dict(zip(tfidf_vect.get_feature_names(), tfidf_vect.idf_))\n",
    "\n",
    "# Second method - averaging weighted words vectors.\n",
    "def ws2meanvec(words, embedding_model, weights):\n",
    "    vectors = []\n",
    "    for w in words:\n",
    "        if(w in embedding_model and w in tfidf):\n",
    "            vectors.append(embedding_model[w]*weights[w])\n",
    " \n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # To avoid empty words causing error. For example, raw sentence is 10/10 at imdb.\n",
    "        return np.zeros(embedding_model.vector_size, dtype=np.float64)\n",
    "    \n",
    "\n",
    "X_train_tl = list(map(partial(ws2meanvec, embedding_model=google_model, weights = tfidf ), corpus_train))\n",
    "X_test_tl = list(map(partial(ws2meanvec, embedding_model=google_model, weights = tfidf), corpus_test))\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X_train_tl, indices_train)\n",
    "pred_test = log_reg.predict(X_test_tl)\n",
    "print(\"Goodle Word2Vec TF-IDF's Accuracy = \", accuracy_score(indices_test, pred_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-b6128436ce8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Goodle Word2Vec TF-IDF's Accuracy = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1284\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1285\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[0;32m--> 565\u001b[0;31m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             _assert_all_finite(array,\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
