{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import nltk\n",
    "from shutil import copyfile\n",
    "\n",
    "#SSL Certificate has fauled\n",
    "#that not in the system certificate store.\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "#PlaintextCorpusReader will use the default nltk.tokenize.sent_tokenize() \n",
    "#and nltk.tokenize.word_tokenize() to split your texts into sentences and words\n",
    "\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581864\n",
      "﻿Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
      "\n",
      "This eBook is for the\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 1----------------------------------------------------------#\n",
    "#Text number 1661 is \"The Adventures of Sherlock Holmes\" by Arthur Conan Doyle, and we can access it as follows.\n",
    "url = \"http://www.gutenberg.org/cache/epub/1661/pg1661.txt\"\n",
    "response = request.urlopen(url)\n",
    "corpus = response.read().decode('utf8')\n",
    "corpus = corpus.replace('\\r', '')\n",
    "length_corpus = len(corpus)\n",
    "print(length_corpus)\n",
    "print(corpus[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newcorpus.nosync/spell_checker_test_set.txt'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make new dir for the corpus.\n",
    "corpusdir = 'newcorpus.nosync/'\n",
    "if not os.path.isdir(corpusdir):\n",
    "    os.mkdir(corpusdir)\n",
    "    \n",
    "copyfile(\"Makefile\", corpusdir + \"Makefile\")\n",
    "copyfile(\"spell_checker_test_set.txt\",corpusdir + \"spell_checker_test_set.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the files into the directory.\n",
    "filename = 'SherlockHolmes.txt'\n",
    "with open(corpusdir+filename, 'w') as f:\n",
    "    print(corpus, file=f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that our corpus do exist and the files are correct.\n",
    "# Key Note:\n",
    "# 1.We split each file into words and we their equality until the penultimate word, since there is one extra '\\n'\n",
    "#in the created file\n",
    "assert open(corpusdir+filename,'r').read().split(' ')[:-1] == corpus.split(' ')[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new corpus by specifying the parameters\n",
    "# (1) directory of the new corpus\n",
    "# (2) the fileids of the corpus\n",
    "# NOTE: in this case the fileids are simply the filenames.\n",
    "# Now the text has been parsed into paragraphs, sentences and words by the default actions\n",
    "# of the PlaintextCorpusReader\n",
    "newcorpus = PlaintextCorpusReader(corpusdir, '.*')\n",
    "os.chdir(corpusdir)\n",
    "#----------------------------------------------END OF STEP 1---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 2----------------------------------------------------------#\n",
    "#----------------------(a)---------------------#\n",
    "#Function used as default argument in parser() function if it is not defined\n",
    "def identity_preprocess(s):\n",
    "    if(isinstance(s, str)):\n",
    "        return s\n",
    "    else: return \"No string was given\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------(b)---------------------#\n",
    "#Function to parse the text file given, line by line\n",
    "def parser(path,preprocess = identity_preprocess):\n",
    "    tokens = []\n",
    "    for line in path.split('\\n'):\n",
    "        tokens+= preprocess(line)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------(c)---------------------#\n",
    "import re\n",
    "import string\n",
    "#Tokenization step, a simple version which includes tokens of lowercase words\n",
    "def tokenize(s):\n",
    "    s_temp = s.strip().lower()\n",
    "    s_temp = re.sub('[^A-Za-z\\n\\s]+', '', s_temp)\n",
    "    s_temp = s_temp.replace('\\n', ' ')\n",
    "    s_temp = \" \".join(s_temp.split())\n",
    "    s_temp = s_temp.split(' ')\n",
    "    s_temp[:] = [item for item in s_temp if item != '']\n",
    "    return s_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'RoCk3', '!', '45.', '!', 'Fell', 'frOm334', '~.', 'heaven']\n",
      "['A', 'RoCk3', '!', '45', '.!', 'Fell', 'frOm334', '~.', 'heaven']\n",
      "['a', 'rock', 'fell', 'from', 'heaven']\n"
     ]
    }
   ],
   "source": [
    "#----------------------(d)---------------------#\n",
    "#Comparing results of our built word tokenizer with a sentence that proves its functionality\n",
    "#with the results given by nltk's word tokenizers\n",
    "#print(\" A RoCk3!45.! Fell frOm334 \\n ~. heaven \")\n",
    "#1.The word_tokenize() function is a wrapper function that calls tokenize() on an\n",
    "#instance of the TreebankWordTokenizer class. It is a simpler, regular-expression \n",
    "#based tokenizer, which splits text on whitespace and punctuation:\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print( tokenizer.tokenize(\" A RoCk3!45.! Fell frOm334 \\n ~. heaven \"))\n",
    "\n",
    "#2.WordPunctTokenizer splits all punctuations into separate tokens:\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(\" A RoCk3!45.! Fell frOm334 \\n ~. heaven \"))\n",
    "\n",
    "#3.Our created tokenizer\n",
    "print(tokenize(\" A RoCk3!45.! Fell     frOm334 \\n ~. heaven \"))\n",
    "#----------------------------------------------END OF STEP 2---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenbergs', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'arthur', 'conan', 'doyle', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere']\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 3----------------------------------------------------------#\n",
    "#Constructing word tokens and alphabet of the new corpus\n",
    "#----------------------(a)---------------------#\n",
    "corpus_preprocessed = newcorpus.raw(newcorpus.fileids()[1])\n",
    "word_tokens = parser(corpus_preprocessed, tokenize)\n",
    "\n",
    "print(word_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------(b)---------------------#\n",
    "def tokenize_2(s):\n",
    "    s_temp = s.strip()\n",
    "    s_temp = \" \".join(s_temp.split())\n",
    "    s_temp = s_temp.split(' ')\n",
    "    return s_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']\n"
     ]
    }
   ],
   "source": [
    "def parser_2(path, preprocess):\n",
    "    alphabet = []\n",
    "    for line in path.split('\\n'):\n",
    "        line = preprocess(line)\n",
    "        for word in line:\n",
    "            alphabet+= list(word)\n",
    "            \n",
    "    alphabet.append(' ')\n",
    "    return set(alphabet)\n",
    "        \n",
    "alphabet_tokens = sorted(parser_2(corpus_preprocessed,tokenize_2))\n",
    "print(alphabet_tokens)\n",
    "\n",
    "#----------------------------------------------END OF STEP 3---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 4----------------------------------------------------------#\n",
    "filename = 'chars.syms'\n",
    "filename =  open(filename, 'w')\n",
    "result = []\n",
    "\n",
    "filename.write('<epsilon>'+ \" \" + str(0)+'\\n')\n",
    "filename.write('<space>'+ \"   \" + str(1)+'\\n')\n",
    "for symbol in range(2,len(alphabet_tokens)):\n",
    "    line = alphabet_tokens[symbol] + \"         \" + str(symbol)+'\\n'\n",
    "    filename.write(line)\n",
    "\n",
    "filename.close()\n",
    "#----------------------------------------------END OF STEP 4---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 5----------------------------------------------------------#\n",
    "#HERE WE CREATE THE TRANDUCER I\n",
    "#----------------------(a)---------------------#\n",
    "filename = 'orth_I.txt'\n",
    "filename = open(filename,'w')\n",
    "\n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "for letter in alphabet:\n",
    "    filename.write(\"0 0 \"+ letter +\" \"+ letter +\" 0\\n\")\n",
    "filename.write(\"0\")\n",
    "\n",
    "filename.close()\n",
    "!make -s orth_I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE WE CREATE THE TRANDUCER E\n",
    "filename = 'orth_E.txt'\n",
    "filename = open(filename,'w')\n",
    "alphabet=\"abcdefghijklmnopqrstuvwxyz\"\n",
    "filename.write('0 1 <epsilon> <epsilon> 0'+'\\n')\n",
    "for i in range(len(alphabet)):\n",
    "    filename.write('0 1 <epsilon> '+alphabet[i]+' '+str(1)+'\\n')#insertion\n",
    "    filename.write('0 1 '+alphabet[i]+' <epsilon> '+str(1)+'\\n')#deletion\n",
    "    for j in range(len(alphabet)):\n",
    "        if alphabet[i]!=alphabet[j]:\n",
    "            filename.write('0 1 '+alphabet[i]+' '+alphabet[j]+' '+str(1)+'\\n')#Replace character by another\n",
    "\n",
    "filename.write(str(1))\n",
    "filename.close()\n",
    "!make -s orth_E\n",
    "!make -s transducer\n",
    "#FINALLY WE CREATE THE TRANDUCER transducer = orth_I | orth_E | orth_I with the Makefile\n",
    "#----------------------------------------------END OF STEP 5---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------STEP 6----------------------------------------------------------#\n",
    "#HERE WE CREATE THE ACCEPTOR/AUTOMATO used to accept all the words of our words_tokens, of the corpus.\n",
    "#One state for each letter of every word-> States will be limited later when we will apply the respective\n",
    "#commands of determinization, minimization, removal of <epsilon> transitions to our orth_acceptor.fst\n",
    "#----------------------(a)---------------------#\n",
    "filename = 'orth_acceptor.txt'\n",
    "acceptor=open(filename, 'w')\n",
    "final_states = []\n",
    "state_count = 0\n",
    "\n",
    "acceptor.write('0 0 <epsilon> 0\\n')\n",
    "for word in word_tokens:\n",
    "    chars = list(word)\n",
    "    if(len(chars) == 1):\n",
    "        arg = ['0',' ',str(state_count+1),' ',chars[0],' ',chars[0],' 0','\\n']\n",
    "        arg = ''.join(arg)\n",
    "        acceptor.write(arg)\n",
    "        state_count += len(chars)\n",
    "        final_states.append(str(state_count))\n",
    "    else:\n",
    "        arg = ['0',' ',str(state_count+1),' ',chars[0],' ',chars[0],' 0','\\n']\n",
    "        arg = ''.join(arg)\n",
    "        acceptor.write(arg)\n",
    "        for j in range(1,len(chars)):\n",
    "            arg = [str(j + state_count),' ',str(j+1 + state_count),' ',chars[j],' ',chars[j],' 0','\\n']\n",
    "            arg = ''.join(arg)\n",
    "            acceptor.write(arg)\n",
    "        state_count += len(chars)\n",
    "        final_states.append(str(state_count))\n",
    "for i in range(0,len(final_states)):\n",
    "    arg = [final_states[i],'\\n']\n",
    "    arg = ''.join(arg)\n",
    "    acceptor.write(arg)\n",
    "acceptor.close()\n",
    "!make -s orth_acceptor\n",
    "!make -s orth_acceptor_processed\n",
    "\n",
    "#----------------------------------------------END OF STEP 6---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t2\tc\tw\t1\r\n",
      "0\r\n",
      "1\t0\tt\tt\r\n",
      "2\t1\ti\ti\r\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 7----------------------------------------------------------#\n",
    "#----------------------(a)---------------------#\n",
    "!make -s orthograph\n",
    "#----------------------(b)---------------------#\n",
    "filename = 'cit.txt'\n",
    "filename = open(filename, 'w')\n",
    "word = \"cit\"\n",
    "state = 0\n",
    "for letter in word:\n",
    "    if letter!='\\n':\n",
    "        filename.write(str(state)+' '+str(state+1)+' '+letter+ '\\n')\n",
    "        state+=1\n",
    "filename.write(str(state)+'\\n')\n",
    "\n",
    "filename.close()\n",
    "!make -s check_cit\n",
    "\n",
    "#----------------------------------------------END OF STEP 7---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['further', 'monitoring', 'biscuits', 'available', 'separate', 'necessary', 'definition', 'receipt', 'remind', 'initials', 'magnificent', 'aunt', 'initial', 'there', 'experiences', 'built', 'totally', 'understand', 'southern', 'definitely']\n",
      "[['futher'], ['monitering'], ['biscits', 'biscutes', 'biscuts', 'bisquits', 'buiscits', 'buiscuts'], ['avaible'], ['seperate'], ['neccesary', 'necesary', 'neccesary', 'necassary', 'necassery', 'neccasary'], ['defenition'], ['receit', 'receite', 'reciet', 'recipt'], ['remine', 'remined'], ['inetials', 'inistals', 'initails', 'initals', 'intials'], ['magnificnet', 'magificent', 'magnifcent', 'magnifecent', 'magnifiscant', 'magnifisent', 'magnificant'], ['annt', 'anut', 'arnt'], ['intial'], ['ther'], ['experances'], ['biult'], ['totaly'], ['undersand', 'undistand'], ['southen'], ['definately', 'difinately']]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 8----------------------------------------------------------#\n",
    "#----------------------(a)---------------------#\n",
    "from lib import *\n",
    "filename = 'spell_checker_test_set.txt'\n",
    "#We take 'spell_checker_test_set.txt', and we split to create 2 lists, the one with the correct words\n",
    "#and the other with the list of the relevant wrong words. We chose randomly to ckeck 20 lines \n",
    "filename = open(filename, 'r')\n",
    "lines = filename.readlines()[20:40]\n",
    "correct_words = []\n",
    "wrong_words =[]\n",
    "for line in lines:\n",
    "    correct_words.append(line.split(':')[0])\n",
    "    wrong_words.append((line.split(':')[1]).split())\n",
    "\n",
    "acceptor = []\n",
    "print(correct_words)\n",
    "print(wrong_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<epsilon>', '<space>', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']\n"
     ]
    }
   ],
   "source": [
    "#We should create the dictionary based on the \"chars.syms\". The position in the dictionary\n",
    "#represents the index in the symbol\n",
    "dictionary = 'chars.syms'\n",
    "dictionary= open(dictionary,'r')\n",
    "lines=dictionary.readlines()\n",
    "dict=[0 for i in range(len(lines))]\n",
    "for line in lines:\n",
    "    matching = line.split()\n",
    "    dict[int(matching[1])]=matching[0]\n",
    "dictionary.close()\n",
    "\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "futher faurtthehrer further\n",
      "monitering  monitoring\n",
      "biscits  biscuits\n",
      "biscutes  biscuits\n",
      "biscuts  biscuits\n",
      "bisquits  biscuits\n",
      "buiscits  biscuits\n",
      "buiscuts  biscuits\n",
      "avaible  available\n",
      "seperate separate separate\n",
      "neccesary  necessary\n",
      "necesary necessssaary necessary\n",
      "neccesary  necessary\n",
      "necassary necessary necessary\n",
      "necassery  necessary\n",
      "neccasary  necessary\n",
      "defenition  definition\n",
      "receit receniptt receipt\n",
      "receite receive receipt\n",
      "reciet  receipt\n",
      "recipt receipt receipt\n",
      "remine  remind\n",
      "remined remained remind\n",
      "inetials initials initials\n",
      "inistals  initials\n",
      "initails  initials\n",
      "initals initials initials\n",
      "intials initials initials\n",
      "magnificnet  magnificent\n",
      "magificent magnificent magnificent\n",
      "magnifcent magnificent magnificent\n",
      "magnifecent magnificent magnificent\n",
      "magnifiscant  magnificent\n",
      "magnifisent magnificent magnificent\n",
      "magnificant magnificent magnificent\n",
      "annt aunt aunt\n",
      "anut 0nut aunt\n",
      "arnt auranttt aunt\n",
      "intial  initial\n",
      "ther t0thto00he0himynrheeerrr there\n",
      "experances  experiences\n",
      "biult  built\n",
      "totaly total totally\n",
      "undersand understand understand\n",
      "undistand  understand\n",
      "southen southern southern\n",
      "definately definitely definitely\n",
      "difinately  definitely\n"
     ]
    }
   ],
   "source": [
    "#Here in file OurResults, we will save the produced words\n",
    "filename = 'OurResults.txt'\n",
    "result = open(filename, 'w')\n",
    "for i in range(len(wrong_words)):\n",
    "    for word in wrong_words[i]:\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #We truncate this file in order to make the other acceptors in the same file\n",
    "        acceptor=open('word_acceptor.txt', 'w')\n",
    "        state = 0\n",
    "        for letter in word:\n",
    "            if letter!='\\n':\n",
    "                acceptor.write(str(state)+' '+str(state+1)+' '+letter +'\\n')\n",
    "                \n",
    "                state+=1\n",
    "        acceptor.write(str(state)+'\\n')\n",
    "        acceptor.close()\n",
    "            #--------------------------------------------------------------------------#\n",
    "        #We use the fst tool in order to create the acceptor for every word\n",
    "        #The method of shortest path was ussed to find the best matches\n",
    "        !make -s unique_word\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #We write the result in a file in order to compare the best words later\n",
    "        acceptor_shortest=open('Acceptor_Shortest.txt', 'r')\n",
    "        lines=acceptor_shortest.readlines()\n",
    "        temp_word=[]\n",
    "\n",
    "        for j in range(2,len(lines)):\n",
    "            chars = lines[j].split()\n",
    "            if(len(chars) > 3):\n",
    "                temp_word.append(chars[3])\n",
    "        if(len(lines) > 1):\n",
    "            chars = lines[0].split()\n",
    "            if(len(chars) > 3):\n",
    "                temp_word.append(chars[3])\n",
    "        #--------------------------------------------------------------------------#\n",
    "        print(word,end =' ')\n",
    "        #Apparently, now in temp_word we have the produced word, which is going to be\n",
    "        #cheked based on our dictionary created in the previous block.\n",
    "        for letter in temp_word[1:(len(temp_word)-1)]:\n",
    "            if int(letter)!=0:\n",
    "                print(dict[int(letter)],end ='')\n",
    "                result.write(dict[int(letter)])\n",
    "        print(' ',end = '')\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #So for each word we save our result bh using this format:\n",
    "        #|word orthograph| + |wrong_word| + |correct_word|\n",
    "        print(correct_words[i])\n",
    "        result.write(' '+word+' '+correct_words[i]+'\\n')\n",
    "\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Our Orthograph gave the following results\n",
      "\n",
      "Corrected Words 15\n",
      "Wrong Words 9\n",
      "There was no matching for 24 words\n"
     ]
    }
   ],
   "source": [
    "#### HERE WE GONNA CHECK THE CORRECTNESS OF OUR ORTHOGRAPH\n",
    "corrected_words=0\n",
    "wrong_words=0\n",
    "no_matching_words=0\n",
    "result=open('OurResults.txt', 'r')\n",
    "words=result.readlines()\n",
    "for word in words:\n",
    "    chars = word.split()\n",
    "    if(len(chars) >2):\n",
    "        if(chars[0] == chars[2] and chars[1]!=chars[2]):\n",
    "            corrected_words+=1\n",
    "        else:\n",
    "            wrong_words +=1\n",
    "    else:\n",
    "        no_matching_words+=1\n",
    "\n",
    "print('\\nOur Orthograph gave the following results\\n')\n",
    "print('Corrected Words ' + str(corrected_words))\n",
    "print('Wrong Words ' + str(wrong_words))\n",
    "print('There was no matching for '+ str(no_matching_words) + ' words')\n",
    "#----------------------------------------------END OF STEP 8---------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['project', 'gutenbergs', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'arthur', 'conan', 'doyle'], ['this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with'], ['almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or'], ['reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included'], ['with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergnet'], ['title', 'the', 'adventures', 'of', 'sherlock', 'holmes'], ['author', 'arthur', 'conan', 'doyle'], ['posting', 'date', 'april', 'ebook'], ['first', 'posted', 'november'], ['language', 'english'], ['start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'the', 'adventures', 'of', 'sherlock', 'holmes'], ['produced', 'by', 'an', 'anonymous', 'project', 'gutenberg', 'volunteer', 'and', 'jose', 'menendez'], ['the', 'adventures', 'of', 'sherlock', 'holmes'], ['by'], ['sir', 'arthur', 'conan', 'doyle'], ['i', 'a', 'scandal', 'in', 'bohemia'], ['ii', 'the', 'redheaded', 'league'], ['iii', 'a', 'case', 'of', 'identity'], ['iv', 'the', 'boscombe', 'valley', 'mystery'], ['v', 'the', 'five', 'orange', 'pips'], ['vi', 'the', 'man', 'with', 'the', 'twisted', 'lip'], ['vii', 'the', 'adventure', 'of', 'the', 'blue', 'carbuncle'], ['viii', 'the', 'adventure', 'of', 'the', 'speckled', 'band'], ['ix', 'the', 'adventure', 'of', 'the', 'engineers', 'thumb'], ['x', 'the', 'adventure', 'of', 'the', 'noble', 'bachelor'], ['xi', 'the', 'adventure', 'of', 'the', 'beryl', 'coronet'], ['xii', 'the', 'adventure', 'of', 'the', 'copper', 'beeches'], ['adventure', 'i', 'a', 'scandal', 'in', 'bohemia'], ['i'], ['to', 'sherlock', 'holmes', 'she', 'is', 'always', 'the', 'woman', 'i', 'have', 'seldom', 'heard']]\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------STEP 9----------------------------------------------------------#\n",
    "#----------------------(a)---------------------#\n",
    "# Initialize word2vec. Context is taken as the 2 previous and 2 next words\n",
    "def parser3(path,preprocess = identity_preprocess):\n",
    "    tokens = []\n",
    "    for line in path.split('\\n'):\n",
    "        s_temp = preprocess(line)\n",
    "        if(s_temp ==[]):continue\n",
    "        tokens.append(s_temp)\n",
    "    return tokens\n",
    "\n",
    "sent_tokens = parser3(corpus_preprocessed, tokenize)\n",
    "print(sent_tokens[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leg [('whoever', 0.40393221378326416), ('fathom', 0.3576420545578003)]\n",
      "presently [('whistle', 0.33856892585754395), ('country', 0.3132840394973755)]\n",
      "murderous [('englishman', 0.5531414747238159), ('expression', 0.3790321946144104)]\n",
      "mud [('flash', 0.4029349386692047), ('throws', 0.390768826007843)]\n",
      "stepping [('trick', 0.46358785033226013), ('stared', 0.4237854778766632)]\n",
      "lens [('pillow', 0.4028383791446686), ('creases', 0.3827398717403412)]\n",
      "but [('and', 0.5507899522781372), ('that', 0.5200991034507751)]\n",
      "fantastic [('misgivings', 0.3805195987224579), ('bride', 0.3748277425765991)]\n",
      "staring [('gazing', 0.4954022765159607), ('drives', 0.40069448947906494)]\n",
      "answering [('jest', 0.42209839820861816), ('tone', 0.35786348581314087)]\n"
     ]
    }
   ],
   "source": [
    "#----------------------(b)---------------------#\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "model = models.Word2Vec( sent_tokens,window=5, size=100, min_count = 2,workers=4)\n",
    "model.train(sent_tokens, total_examples=len(sent_tokens), epochs=1000)\n",
    "\n",
    "# get ordered vocabulary list\n",
    "voc = model.wv.index2word\n",
    "\n",
    "# get vector size\n",
    "dim = model.vector_size\n",
    "\n",
    "words_to_check = random.sample(voc, 10)\n",
    "for word in words_to_check:\n",
    "    sim = model.wv.most_similar(word,topn=2)\n",
    "    print(word,sim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
