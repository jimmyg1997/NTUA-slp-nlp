{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------------------------------STEP 4 ----------------------------------------------------#\n",
    "#-------------------(4.1) - 1-------------------#\n",
    "#Try to source path.sh#\n",
    "#This recipe requires to set up $KALDI_ROOT variable so it can use Kaldi \n",
    "#binaries and scripts from $KALDI_ROOT/egs/wsj/s5/.\n",
    "import os\n",
    "kaldi = !mdfind kind:folder \"kaldi\"\n",
    "kaldi = kaldi[len(kaldi) - 1]\n",
    "\n",
    "kaldi_ = kaldi + \"/egs/wsj/s5/path.sh\"\n",
    "\n",
    "bashCommand = \". \" + kaldi_\n",
    "os.system(bashCommand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: local: File exists\r\n"
     ]
    }
   ],
   "source": [
    "#-------------------(4.1) - 2,3-------------------#\n",
    "#Create softlinks#\n",
    "\n",
    "kaldi_softlink1 = kaldi[0] + \"/egs/wsj/s5/steps\"\n",
    "bashCommand = \"ln -s \" + kaldi_softlink1 + \" steps\"\n",
    "os.system(bashCommand)\n",
    "\n",
    "\n",
    "kaldi_softlink2 = kaldi[0] + \"/egs/wsj/s5/utils\"\n",
    "bashCommand = \"ln -s \" + kaldi_softlink2 + \" utils\"\n",
    "os.system(bashCommand)\n",
    "\n",
    "\n",
    "\n",
    "!mkdir local\n",
    "\n",
    "kaldi_softlink3 = \"../steps/score_kaldi.sh\"\n",
    "bashCommand = \"ln -s \" + kaldi_softlink3 + \" score_kaldi.sh\"\n",
    "os.system(bashCommand)\n",
    "\n",
    "!mv score_kaldi.sh local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------(4.2) - 1 - a-------------------#\n",
    "\n",
    "filename = open(\"data/local/dict/silence_phones.txt\",'w')\n",
    "filename.write('sil\\n')\n",
    "filename.close()\n",
    "\n",
    "filename = open(\"data/local/dict/optional_silence.txt\",'w')\n",
    "filename.write('sil\\n')\n",
    "filename.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------(4.2) - 1 - b-------------------#\n",
    "lexicon = \"slp_lab2_data/lexicon.txt\"\n",
    "lexicon = open(lexicon,'r')\n",
    "lines_lexicon = lexicon.readlines()\n",
    "lexicon.close()\n",
    "\n",
    "phones = \"data/local/dict/nonsilence_phones.txt\"\n",
    "phones = open(phones,'w')\n",
    "phonemes = []\n",
    "for line in lines_lexicon:\n",
    "    sentence_phonemes = line.split()[1:]\n",
    "    phonemes += sentence_phonemes\n",
    "\n",
    "phonemes = sorted(list(set(phonemes)))\n",
    "for phonem in phonemes:\n",
    "    if(phonem == 'sil'):continue\n",
    "    phones.write(phonem + '\\n')\n",
    "phones.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------(4.2) - 1 - c-------------------#\n",
    "lexicon = \"data/local/dict/lexicon.txt\"\n",
    "lexicon = open(lexicon,'w')\n",
    "\n",
    "for phonem in phonemes:\n",
    "    lexicon.write(phonem + ' '+ phonem + '\\n')\n",
    "lexicon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------(4.2) - 1 - d-------------------\n",
    "import shutil \n",
    "import os \n",
    "from collections import defaultdict \n",
    "\n",
    "destination_directory = kaldi + \"/egs/usc/data\" \n",
    "\n",
    "for file in os.listdir(destination_directory): \n",
    "    if(file == \".DS_Store\"): continue\n",
    "\n",
    "    filename_source_destination = open(destination_directory + '/'+file + \"/text\", 'r')\n",
    "    lines_source_destination = filename_source_destination.readlines()\n",
    "    filename_source_destination.close()\n",
    "    line_pointer = 0\n",
    "\n",
    "    for line in lines_source_destination:\n",
    "        line_splitted = line.split()\n",
    "        utterance_id = line_splitted[0]\n",
    "        sentence = \"<s> \" + ' '.join(line_splitted[1:]) + \" </s>\\n\"\n",
    "        lines_source_destination[line_pointer] = sentence\n",
    "        line_pointer+=1\n",
    "\n",
    "    with open(\"data/local/dict/\" +\"/lm_\"+file+\".text\", 'w') as filename_source_destination:\n",
    "        filename_source_destination.writelines( lines_source_destination )\n",
    "\n",
    "    filename_source_destination.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------(4.2) - 1 - e-------------------#\n",
    "for file in os.listdir(destination_directory):\n",
    "    if(file == \".DS_Store\"): continue\n",
    "        \n",
    "    filename_source_destination = open(\"data/local/dict/\"+\"/extra_questions.txt\", 'w')\n",
    "    filename_source_destination.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n",
      "LOGFILE:/dev/null\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$bin/ngt -i=\"$inpfile\" -n=$order -gooout=y -o=\"$gzip -c > $tmpdir/ngram.${sdict}.gz\" -fd=\"$tmpdir/$sdict\" $dictionary $additional_parameters >> $logfile 2>&1\n",
      "$scr/build-sublm.pl $verbose $prune $prune_thr_str $smoothing \"$additional_smoothing_parameters\" --size $order --ngrams \"$gunzip -c $tmpdir/ngram.${sdict}.gz\" -sublm $tmpdir/lm.$sdict $additional_parameters >> $logfile 2>&1\n"
     ]
    }
   ],
   "source": [
    "### -------------------(4.2) - 2 -----------------------#\n",
    "##Notice that build-lm.sh produces a LM file train.ilm.gz that is NOT in the\n",
    "##final ARPA format, but in an intermediate format called iARPA, that is recognized by the compile-lm\n",
    "##command and by the Moses SMT decoder running with IRSTLM. To convert the file into the standard ARPA\n",
    "##format you can use the command:  compile-lm train.ilm.gz --text yes train.lm\n",
    "##ENVIRONMENT /Users/jimmyg1997/kaldi/tools/irstlm/bin\n",
    "os.chdir(\"/Users/jimmyg1997/Desktop/SLP_lab2/\")\n",
    "\n",
    "!./4_2_2.sh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpfile: lm_train1.ilm.gz\n",
      "outfile: lm_train1.lm\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to lm_train1.lm\n",
      "inpfile: lm_train1.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: lm_train2.ilm.gz\n",
      "outfile: lm_train2.lm\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to lm_train2.lm\n",
      "inpfile: lm_train2.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: lm_test1.ilm.gz\n",
      "outfile: lm_test1.lm\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to lm_test1.lm\n",
      "inpfile: lm_test1.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: lm_test2.ilm.gz\n",
      "outfile: lm_test2.lm\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to lm_test2.lm\n",
      "inpfile: lm_test2.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: lm_dev1.ilm.gz\n",
      "outfile: lm_dev1.lm\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to lm_dev1.lm\n",
      "inpfile: lm_dev1.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to /dev/stdout\n",
      "inpfile: lm_dev2.ilm.gz\n",
      "outfile: lm_dev2.lm\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to lm_dev2.lm\n",
      "inpfile: lm_dev2.ilm.gz\n",
      "outfile: /dev/stdout\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Saving in txt format to /dev/stdout\n"
     ]
    }
   ],
   "source": [
    "### -------------------(4.2) - 3 -----------------------#\n",
    "!./4_2_3.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./utils/prepare_lang.sh data/local/dict <oov> data/local/lm_tmp data/lang\n",
      "Checking data/local/dict/silence_phones.txt ...\n",
      "--> reading data/local/dict/silence_phones.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> data/local/dict/silence_phones.txt is OK\n",
      "\n",
      "Checking data/local/dict/optional_silence.txt ...\n",
      "--> reading data/local/dict/optional_silence.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> data/local/dict/optional_silence.txt is OK\n",
      "\n",
      "Checking data/local/dict/nonsilence_phones.txt ...\n",
      "--> reading data/local/dict/nonsilence_phones.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> data/local/dict/nonsilence_phones.txt is OK\n",
      "\n",
      "Checking disjoint: silence_phones.txt, nonsilence_phones.txt\n",
      "--> disjoint property is OK.\n",
      "\n",
      "Checking data/local/dict/lexicon.txt\n",
      "--> reading data/local/dict/lexicon.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> data/local/dict/lexicon.txt is OK\n",
      "\n",
      "Checking data/local/dict/lexiconp.txt\n",
      "--> reading data/local/dict/lexiconp.txt\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> data/local/dict/lexiconp.txt is OK\n",
      "\n",
      "Checking lexicon pair data/local/dict/lexicon.txt and data/local/dict/lexiconp.txt\n",
      "--> lexicon pair data/local/dict/lexicon.txt and data/local/dict/lexiconp.txt match\n",
      "\n",
      "Checking data/local/dict/extra_questions.txt ...\n",
      "--> data/local/dict/extra_questions.txt is empty (this is OK)\n",
      "--> SUCCESS [validating dictionary directory data/local/dict]\n",
      "\n",
      "fstaddselfloops data/lang/phones/wdisambig_phones.int data/lang/phones/wdisambig_words.int \n",
      "prepare_lang.sh: validating output directory\n",
      "utils/validate_lang.pl data/lang\n",
      "Checking data/lang/phones.txt ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> data/lang/phones.txt is OK\n",
      "\n",
      "Checking words.txt: #0 ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> data/lang/words.txt is OK\n",
      "\n",
      "Checking disjoint: silence.txt, nonsilence.txt, disambig.txt ...\n",
      "--> silence.txt and nonsilence.txt are disjoint\n",
      "--> silence.txt and disambig.txt are disjoint\n",
      "--> disambig.txt and nonsilence.txt are disjoint\n",
      "--> disjoint property is OK\n",
      "\n",
      "Checking sumation: silence.txt, nonsilence.txt, disambig.txt ...\n",
      "--> found no unexplainable phones in phones.txt\n",
      "\n",
      "Checking data/lang/phones/context_indep.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 5 entry/entries in data/lang/phones/context_indep.txt\n",
      "--> data/lang/phones/context_indep.int corresponds to data/lang/phones/context_indep.txt\n",
      "--> data/lang/phones/context_indep.csl corresponds to data/lang/phones/context_indep.txt\n",
      "--> data/lang/phones/context_indep.{txt, int, csl} are OK\n",
      "\n",
      "Checking data/lang/phones/nonsilence.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 160 entry/entries in data/lang/phones/nonsilence.txt\n",
      "--> data/lang/phones/nonsilence.int corresponds to data/lang/phones/nonsilence.txt\n",
      "--> data/lang/phones/nonsilence.csl corresponds to data/lang/phones/nonsilence.txt\n",
      "--> data/lang/phones/nonsilence.{txt, int, csl} are OK\n",
      "\n",
      "Checking data/lang/phones/silence.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 5 entry/entries in data/lang/phones/silence.txt\n",
      "--> data/lang/phones/silence.int corresponds to data/lang/phones/silence.txt\n",
      "--> data/lang/phones/silence.csl corresponds to data/lang/phones/silence.txt\n",
      "--> data/lang/phones/silence.{txt, int, csl} are OK\n",
      "\n",
      "Checking data/lang/phones/optional_silence.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 1 entry/entries in data/lang/phones/optional_silence.txt\n",
      "--> data/lang/phones/optional_silence.int corresponds to data/lang/phones/optional_silence.txt\n",
      "--> data/lang/phones/optional_silence.csl corresponds to data/lang/phones/optional_silence.txt\n",
      "--> data/lang/phones/optional_silence.{txt, int, csl} are OK\n",
      "\n",
      "Checking data/lang/phones/disambig.{txt, int, csl} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 2 entry/entries in data/lang/phones/disambig.txt\n",
      "--> data/lang/phones/disambig.int corresponds to data/lang/phones/disambig.txt\n",
      "--> data/lang/phones/disambig.csl corresponds to data/lang/phones/disambig.txt\n",
      "--> data/lang/phones/disambig.{txt, int, csl} are OK\n",
      "\n",
      "Checking data/lang/phones/roots.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 41 entry/entries in data/lang/phones/roots.txt\n",
      "--> data/lang/phones/roots.int corresponds to data/lang/phones/roots.txt\n",
      "--> data/lang/phones/roots.{txt, int} are OK\n",
      "\n",
      "Checking data/lang/phones/sets.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 41 entry/entries in data/lang/phones/sets.txt\n",
      "--> data/lang/phones/sets.int corresponds to data/lang/phones/sets.txt\n",
      "--> data/lang/phones/sets.{txt, int} are OK\n",
      "\n",
      "Checking data/lang/phones/extra_questions.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 9 entry/entries in data/lang/phones/extra_questions.txt\n",
      "--> data/lang/phones/extra_questions.int corresponds to data/lang/phones/extra_questions.txt\n",
      "--> data/lang/phones/extra_questions.{txt, int} are OK\n",
      "\n",
      "Checking data/lang/phones/word_boundary.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 165 entry/entries in data/lang/phones/word_boundary.txt\n",
      "--> data/lang/phones/word_boundary.int corresponds to data/lang/phones/word_boundary.txt\n",
      "--> data/lang/phones/word_boundary.{txt, int} are OK\n",
      "\n",
      "Checking optional_silence.txt ...\n",
      "--> reading data/lang/phones/optional_silence.txt\n",
      "--> data/lang/phones/optional_silence.txt is OK\n",
      "\n",
      "Checking disambiguation symbols: #0 and #1\n",
      "--> data/lang/phones/disambig.txt has \"#0\" and \"#1\"\n",
      "--> data/lang/phones/disambig.txt is OK\n",
      "\n",
      "Checking topo ...\n",
      "\n",
      "Checking word_boundary.txt: silence.txt, nonsilence.txt, disambig.txt ...\n",
      "--> data/lang/phones/word_boundary.txt doesn't include disambiguation symbols\n",
      "--> data/lang/phones/word_boundary.txt is the union of nonsilence.txt and silence.txt\n",
      "--> data/lang/phones/word_boundary.txt is OK\n",
      "\n",
      "Checking word-level disambiguation symbols...\n",
      "--> data/lang/phones/wdisambig.txt exists (newer prepare_lang.sh)\n",
      "Checking word_boundary.int and disambig.int\n",
      "--> generating a 68 word sequence\n",
      "--> resulting phone sequence from L.fst corresponds to the word sequence\n",
      "--> L.fst is OK\n",
      "--> generating a 91 word sequence\n",
      "--> resulting phone sequence from L_disambig.fst corresponds to the word sequence\n",
      "--> L_disambig.fst is OK\n",
      "\n",
      "Checking data/lang/oov.{txt, int} ...\n",
      "--> text seems to be UTF-8 or ASCII, checking whitespaces\n",
      "--> text contains only allowed whitespaces\n",
      "--> 1 entry/entries in data/lang/oov.txt\n",
      "--> data/lang/oov.int corresponds to data/lang/oov.txt\n",
      "--> data/lang/oov.{txt, int} are OK\n",
      "\n",
      "--> data/lang/L.fst is olabel sorted\n",
      "--> data/lang/L_disambig.fst is olabel sorted\n",
      "--> SUCCESS [validating lang directory data/lang]\n"
     ]
    }
   ],
   "source": [
    "### -------------------(4.2) - 4 -----------------------#\n",
    "# This script prepares a directory such as data/lang/, in the standard format,\n",
    "# given a source directory containing a dictionary lexicon.txt in a form like:\n",
    "# word phone1 phone2 ... phoneN\n",
    "# per line (alternate prons would be separate lines), or a dictionary with probabilities\n",
    "# called lexiconp.txt in a form:\n",
    "# word pron-prob phone1 phone2 ... phoneN\n",
    "#\n",
    "#This step is considered as the second part of the language-data preparation. \n",
    "#After creating the “local/dict” directory, the user needs to create another required directory, \n",
    "#called “lang”. This directory can be generated by using “prepare_lang.sh” script that is provided by \n",
    "#Kaldi toolkit. This directory can be prepared as follows:\n",
    "#https://blogs.cornell.edu/finitestatecompling/2016/09/26/utilsprepare_lang-sh/\n",
    "\n",
    "\n",
    "#L_disambiguate.fst:In general, you need to have disambiguation symbols when you have one word that is \n",
    "#a prefix of another (cat and cats in the same lexicon would need to have cat being pronounced “k ae t #1”) \n",
    "#or a homophone of another word (red: “r eh d #1”, read: “r eh d #2”). If you don’t have these then the models \n",
    "#become nondeterministic.\n",
    "\n",
    "\n",
    "#Note 1: utils/validate_dict_dir.pl data/local/dict check (like substep of prepare_lang)\n",
    "\n",
    "\n",
    "!./4_2_4.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing train, dev and test data\n",
      "Preparing language models for test\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_train1.fst \n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 1 to 1\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_dev1.fst \n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 1 to 1\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_test1.fst \n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 1 to 1\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_train2.fst \n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\2-grams: section.\n",
      "WARNING (arpa2fst[5.5.139~1-0e5d75]:ConsumeNGram():arpa-lm-compiler.cc:313) line 52 [-2.85185\t<s> <s>] skipped: n-gram has invalid BOS/EOS placement\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 42 to 42\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_dev2.fst \n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\2-grams: section.\n",
      "WARNING (arpa2fst[5.5.139~1-0e5d75]:ConsumeNGram():arpa-lm-compiler.cc:313) line 52 [-1.97068\t<s> <s>] skipped: n-gram has invalid BOS/EOS placement\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 42 to 42\n",
      "arpa2fst --disambig-symbol=#0 --read-symbol-table=data/lang_test/words.txt - data/lang_test/G_test2.fst \n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:94) Reading \\data\\ section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\1-grams: section.\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:Read():arpa-file-parser.cc:149) Reading \\2-grams: section.\n",
      "WARNING (arpa2fst[5.5.139~1-0e5d75]:ConsumeNGram():arpa-lm-compiler.cc:313) line 52 [-1.97299\t<s> <s>] skipped: n-gram has invalid BOS/EOS placement\n",
      "LOG (arpa2fst[5.5.139~1-0e5d75]:RemoveRedundantStates():arpa-lm-compiler.cc:359) Reduced num-states from 42 to 42\n",
      "Succeeded in formatting data.\n"
     ]
    }
   ],
   "source": [
    "### -------------------(4.2) - 5 -----------------------#\n",
    "!./timit_format_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################\n",
      "Calculating perplexity for evaluation data unigram model\n",
      "########################################################\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "pruning LM with thresholds: \n",
      "\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_dev1.lm\n",
      "outfile: lm_dev1.lm.blm\n",
      "evalfile: ../dict/lm_dev.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Start Eval\n",
      "OOV code: 42\n",
      "%% Nw=6332 PP=32.52 PPwp=0.00 Nbo=0 Noov=0 OOV=0.00%\n",
      "#######################################################\n",
      "Calculating perplexity for evaluation data bigram model\n",
      "#######################################################\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "pruning LM with thresholds: \n",
      " 1e-06\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_dev2.lm\n",
      "outfile: lm_dev2.lm.blm\n",
      "evalfile: ../dict/lm_dev.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Start Eval\n",
      "OOV code: 42\n",
      "%% Nw=6332 PP=15.26 PPwp=0.00 Nbo=0 Noov=0 OOV=0.00%\n",
      "##################################################\n",
      "Calculating perplexity for test data unigram model\n",
      "##################################################\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "pruning LM with thresholds: \n",
      "\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_test1.lm\n",
      "outfile: lm_test1.lm.blm\n",
      "evalfile: ../dict/lm_test.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Start Eval\n",
      "OOV code: 42\n",
      "%% Nw=6148 PP=32.00 PPwp=0.00 Nbo=0 Noov=0 OOV=0.00%\n",
      "#################################################\n",
      "Calculating perplexity for test data bigram model\n",
      "#################################################\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "pruning LM with thresholds: \n",
      " 1e-06\n",
      "DEBUG_LEVEL:0/1 Everything OK\n",
      "inpfile: lm_test2.lm\n",
      "outfile: lm_test2.lm.blm\n",
      "evalfile: ../dict/lm_test.text\n",
      "loading up to the LM level 1000 (if any)\n",
      "dub: 10000000\n",
      "OOV code is 42\n",
      "OOV code is 42\n",
      "Start Eval\n",
      "OOV code: 42\n",
      "%% Nw=6148 PP=14.58 PPwp=0.00 Nbo=0 Noov=0 OOV=0.00%\n"
     ]
    }
   ],
   "source": [
    "#Computation of the Perplexity\n",
    "#With the estimated LM, we can now compute the perplexity of any text contained in \"test.txt\" \n",
    "#by running the commands below.\n",
    "#To be compliant with the training data actually used to estimate the LM, start and end symbols \n",
    "#are added to the text as well.\n",
    "\n",
    "!./4_2_appendix.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------(4.3) - 1-------------------#\n",
    "\n",
    "#!./4_3_1.sh\n",
    "#!./4_3_appendix.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps/decode.sh --nj 3 --cmd run.pl exp/mono0a/graph_nosp_tgpr data/dev exp/mono0a/decode_dev\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/mono0a/graph_nosp_tgpr exp/mono0a/decode_dev\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono0a/decode_dev/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(8,43,510) and mean=268.0\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono0a/decode_dev/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/dev exp/mono0a/graph_nosp_tgpr exp/mono0a/decode_dev\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "steps/decode.sh --nj 3 --cmd run.pl exp/mono0a/graph_nosp_tgpr data/test exp/mono0a/decode_test\n",
      "decode.sh: feature type is delta\n",
      "steps/diagnostic/analyze_lats.sh --cmd run.pl exp/mono0a/graph_nosp_tgpr exp/mono0a/decode_test\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono0a/decode_test/log/analyze_alignments.log\n",
      "Overall, lattice depth (10,50,90-percentile)=(8,41,530) and mean=242.6\n",
      "steps/diagnostic/analyze_lats.sh: see stats in exp/mono0a/decode_test/log/analyze_lattice_depth_stats.log\n",
      "local/score_kaldi.sh --cmd run.pl data/test exp/mono0a/graph_nosp_tgpr exp/mono0a/decode_test\n",
      "local/score_kaldi.sh: scoring with word insertion penalty=0.0,0.5,1.0\n",
      "%WER 50.12 [ 3082 / 6149, 168 ins, 1277 del, 1637 sub ] exp/mono0a/decode_dev/wer_4_0.0\n",
      "%WER 47.64 [ 2841 / 5964, 173 ins, 1070 del, 1598 sub ] exp/mono0a/decode_test/wer_3_0.5\n",
      "%WER 41.62 [ 2559 / 6149, 309 ins, 670 del, 1580 sub ] exp/tri1/decode_dev_1kshort/wer_6_0.5\n",
      "%WER 39.76 [ 2371 / 5964, 271 ins, 705 del, 1395 sub ] exp/tri1/decode_test_1kshort/wer_6_1.0\n"
     ]
    }
   ],
   "source": [
    "### -------------------(4.4) - 1-------------------#\n",
    "!./4_4.sh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
